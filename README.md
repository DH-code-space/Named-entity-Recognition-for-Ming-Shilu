# Named-entity-Recognition-for-Ming-Shilu

## Dependencies

```
pip install -r requirements.txt
```

## Introduction

We leverage Large Language Models (LLMs) to create annotated data for the Named Entity Recognition (NER) task in classical Chinese through prompt engineering and in-context learning techniques. Furthermore, we introduce a novel approach called Knowledge-based Correction, which automatically corrects certain inaccuracies arising from LLMs. Our experiments demonstrate that our model can reach performance comparable to that of GPT4 in this task by utilizing just 500 training paragraphs generated by GPT4. This highlights the significant potential of LLMs in scenarios where labelled data is scarce or nonexistent.

## Usage

Put the [model](https://github.com/youchen0620/Named-entity-Recognition-for-Ming-Shilu/releases/tag/model) into this folder.

Create **Dataset/testing_data.txt** where you can put what you want to recognize the named-entities.

Then use **Code/predict.py** to predict.

The result will be showed in ***Dataset/testing_data_ner.txt**

## Finding out the performance of LLMs

1. Use **Code/llm_processing.ipynb** to get the annotation of **Dataset/testing_data.txt** from LLMs. (**Dataset/testing_data_gpt3.5output.txt**, **Dataset/testing_data_gpt4output.txt**)

2. Use **Code/calculate_F1_no_postprocessing.py** to evaluate the annotation quality of LLMs.

3. Use **Code/llm_postprocessing.py** to automatically correct some mistakes caused by LLMs.

4. Use **Code/calculate_F1_unweighted.py** and **Code/calculate_F1_weighted.py** to evaluate the annotation quality after Knowledge-based Correction.

## Training the Model

1. Use **Code/llm_processing.ipynb** to get the annotation of **Dataset/training_data.txt** from LLMs. (**Dataset/training_data_gpt4output.txt**)

2. Use **Code/llm_postprocessing.py** to automatically correct some mistakes caused by LLMs.

3. Use **Code/create_flair_dataset.py** to create the dataset in **Dataset/model_training** folder.

4. Use **Code/train_flair_model.py** to training the model.
